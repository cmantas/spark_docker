version: '2' 
services:
  namenode:
    image: hadoop_base
    container_name: namenode
    command: /bin/bash -c '/run_namenode.sh'
    volumes:
      - ./data/namenode:/hadoop/dfs/name
      - ./logs/:/hadoop/logs
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./resources/hadoop.env
    network_mode: "host"
  spark-master:
    image: base_spark
    container_name: spark-master
    environment:
      -  ENABLE_INIT_DAEMON=false
    network_mode: "host"
    command: /bin/bash -c '/spark/sbin/start-master.sh && tail -f /dev/null'
  jupyter-pyspark:
     image: jupyter_pyspark
     container_name: jupyter-pyspark
     network_mode: "host"
     command: "start-notebook.sh --NotebookApp.token='' --port=8889"
  hdfs-browser:
    image: gethue/hue:latest
    network_mode: "host"
    command: /bin/bash -c 'build/env/bin/hue runserver_plus 0.0.0.0:8888'
    #environment:
    #  - NAMENODE_HOST=localhost
    volumes:
      - ./logs/:/logs
    command: /bin/bash -c 'build/env/bin/hue runserver_plus 0.0.0.0:8888 &>/logs/hue.out'
